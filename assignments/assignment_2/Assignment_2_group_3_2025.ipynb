{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "322672e4-e393-46a4-b9a9-ca335e60db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pickup_month  pickup_day  pickup_dayofweek  pickup_location_code  \\\n",
      "0                1           1                 5                     2   \n",
      "1                1           1                 5                     2   \n",
      "2                1           1                 5                     2   \n",
      "3                1           1                 5                     2   \n",
      "4                1           1                 5                     2   \n",
      "...            ...         ...               ...                   ...   \n",
      "2008             6          30                 4                     3   \n",
      "2009             6          30                 4                     2   \n",
      "2010             6          30                 4                     2   \n",
      "2011             6          30                 4                     2   \n",
      "2012             6          30                 4                     2   \n",
      "\n",
      "      dropoff_location_code  trip_distance  trip_length  fare_amount  \\\n",
      "0                         4          21.00         2037         52.0   \n",
      "1                         1          16.29         1520         45.0   \n",
      "2                         6          12.70         1462         36.5   \n",
      "3                         6           8.70         1210         26.0   \n",
      "4                         6           5.56          759         17.5   \n",
      "...                     ...            ...          ...          ...   \n",
      "2008                      4           9.50         1989         31.0   \n",
      "2009                      4          19.80         2368         52.0   \n",
      "2010                      4          17.48         2822         52.0   \n",
      "2011                      6          12.76         1083         34.5   \n",
      "2012                      0          17.54         1711         48.0   \n",
      "\n",
      "      total_amount  \n",
      "0            69.99  \n",
      "1            54.30  \n",
      "2            37.80  \n",
      "3            32.76  \n",
      "4            18.80  \n",
      "...            ...  \n",
      "2008         40.84  \n",
      "2009         58.34  \n",
      "2010         63.34  \n",
      "2011         44.75  \n",
      "2012         54.84  \n",
      "\n",
      "[2013 rows x 9 columns]\n",
      "      pickup_month  trip_distance\n",
      "0                1          21.00\n",
      "1                1          16.29\n",
      "2                1          12.70\n",
      "3                1           8.70\n",
      "4                1           5.56\n",
      "...            ...            ...\n",
      "2008             6           9.50\n",
      "2009             6          19.80\n",
      "2010             6          17.48\n",
      "2011             6          12.76\n",
      "2012             6          17.54\n",
      "\n",
      "[2013 rows x 2 columns]\n",
      "pickup_month\n",
      "1    13.039750\n",
      "2    13.145227\n",
      "3    12.702256\n",
      "4    12.808012\n",
      "6    12.964744\n",
      "Name: trip_distance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "nyc_taxis=pd.read_csv(\"nyc_taxis.csv\")\n",
    "\n",
    "#Call the Database\n",
    "nyc_taxis=nyc_taxis[[\"pickup_month\", \"pickup_day\", \"pickup_dayofweek\", \"pickup_location_code\", \"dropoff_location_code\", \"trip_distance\", \"trip_length\", \"fare_amount\", \"total_amount\"]]\n",
    "print(nyc_taxis)\n",
    "\n",
    "#The average distance of the trip (trip_distance) for each month (pickup_month)\n",
    "\n",
    "#Pick up month and trip distance\n",
    "nyc_taxis_1=nyc_taxis[[\"pickup_month\",\"trip_distance\"]]\n",
    "print(nyc_taxis_1)\n",
    "\n",
    "# Group by pickup_month and calculate mean trip_distance\n",
    "monthly_avg_distance = nyc_taxis_1.groupby('pickup_month')['trip_distance'].mean()\n",
    "print(monthly_avg_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f357a98-5538-435b-b093-53a1e2701648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pickup_month  pickup_location_code\n",
      "0                1                     2\n",
      "1                1                     2\n",
      "2                1                     2\n",
      "3                1                     2\n",
      "4                1                     2\n",
      "...            ...                   ...\n",
      "2004             6                     2\n",
      "2009             6                     2\n",
      "2010             6                     2\n",
      "2011             6                     2\n",
      "2012             6                     2\n",
      "\n",
      "[724 rows x 2 columns]\n",
      "\n",
      "Total of trips from JFK for each day of the month:\n",
      "   pickup_month  total_trips\n",
      "0             1          330\n",
      "1             2           60\n",
      "2             3          162\n",
      "3             4           63\n",
      "4             6          109\n"
     ]
    }
   ],
   "source": [
    "#The total of trips from JFK for each day of the month\n",
    "nyc_taxis_2 = nyc_taxis[nyc_taxis['pickup_location_code'] == 2][['pickup_month', 'pickup_location_code']]\n",
    "print(nyc_taxis_2)\n",
    "\n",
    "#Trips per month but only about JFK airport\n",
    "trips_per_month = nyc_taxis[nyc_taxis['pickup_location_code'] == 2].groupby('pickup_month')['pickup_location_code'].count()\n",
    "\n",
    "trips_per_month = trips_per_month.reset_index(name='total_trips')\n",
    "print(\"\\nTotal of trips from JFK for each day of the month:\")\n",
    "print(trips_per_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95b2ed12-1fb9-4690-917b-ed088774bb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average velocity for each day of the week:\n",
      "   pickup_dayofweek  avg_velocity\n",
      "0                 1      0.036826\n",
      "1                 2      0.007596\n",
      "2                 3      0.081112\n",
      "3                 4      0.025408\n",
      "4                 5      0.042695\n",
      "5                 6      0.102427\n",
      "6                 7      0.039451\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average velocity (speed) for each day of the week (pickup_dayofweek)\n",
    "nyc_taxis[\"velocity\"] = nyc_taxis[\"trip_distance\"] / nyc_taxis[\"trip_length\"]\n",
    "\n",
    "# Calculate the average velocity by pickup_dayofweek using groupby\n",
    "avg_velocity_by_day = nyc_taxis.groupby('pickup_dayofweek')['velocity'].mean()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "velocity_summary_df = avg_velocity_by_day.reset_index(name='avg_velocity')\n",
    "\n",
    "# Sort by day of week to ensure order\n",
    "velocity_summary_df = velocity_summary_df.sort_values('pickup_dayofweek')\n",
    "\n",
    "# Print the result\n",
    "print(\"The average velocity for each day of the week:\")\n",
    "print(velocity_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "120f5bb6-b56a-455f-97ec-1bc79a8464e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average velocity for each day of the week (km/h):\n",
      "   pickup_dayofweek  avg_velocity_kmh\n",
      "0                 1            213.35\n",
      "1                 2             44.01\n",
      "2                 3            469.93\n",
      "3                 4            147.20\n",
      "4                 5            247.36\n",
      "5                 6            593.42\n",
      "6                 7            228.57\n"
     ]
    }
   ],
   "source": [
    "# Now calculate velocity with kilometers per hour:\n",
    "# Distance: miles to kilometers (1 mile = 1.60934 km)\n",
    "#Time: seconds to hours (1 hour = 3600 seconds)\n",
    "nyc_taxis[\"velocity_kmh\"] = (nyc_taxis[\"trip_distance\"] * 1.60934) / (nyc_taxis[\"trip_length\"] / 3600)\n",
    "\n",
    "# Calculate average velocity by day using groupby\n",
    "avg_velocity_by_day = nyc_taxis.groupby('pickup_dayofweek')['velocity_kmh'].mean()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "velocity_summary_df = avg_velocity_by_day.reset_index(name='avg_velocity_kmh')\n",
    "\n",
    "# Sort by day of the week\n",
    "velocity_summary_df = velocity_summary_df.sort_values('pickup_dayofweek')\n",
    "\n",
    "# Round to 2 decimal places\n",
    "velocity_summary_df['avg_velocity_kmh'] = velocity_summary_df['avg_velocity_kmh'].round(2)\n",
    "\n",
    "print(\"The average velocity for each day of the week (km/h):\")\n",
    "print(velocity_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29d338be-5d21-448c-b5b6-29ea2e02b5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n",
      "16\n",
      "17\n",
      "Fizz\n",
      "19\n",
      "Buzz\n",
      "Fizz\n",
      "22\n",
      "23\n",
      "Fizz\n",
      "Buzz\n",
      "26\n",
      "Fizz\n",
      "28\n",
      "29\n",
      "FizzBuzz\n",
      "31\n",
      "32\n",
      "Fizz\n",
      "34\n",
      "Buzz\n",
      "Fizz\n",
      "37\n",
      "38\n",
      "Fizz\n",
      "Buzz\n",
      "41\n",
      "Fizz\n",
      "43\n",
      "44\n",
      "FizzBuzz\n",
      "46\n",
      "47\n",
      "Fizz\n",
      "49\n",
      "Buzz\n",
      "Fizz\n",
      "52\n",
      "53\n",
      "Fizz\n",
      "Buzz\n",
      "56\n",
      "Fizz\n",
      "58\n",
      "59\n",
      "FizzBuzz\n",
      "61\n",
      "62\n",
      "Fizz\n",
      "64\n",
      "Buzz\n",
      "Fizz\n",
      "67\n",
      "68\n",
      "Fizz\n",
      "Buzz\n",
      "71\n",
      "Fizz\n",
      "73\n",
      "74\n",
      "FizzBuzz\n",
      "76\n",
      "77\n",
      "Fizz\n",
      "79\n",
      "Buzz\n",
      "Fizz\n",
      "82\n",
      "83\n",
      "Fizz\n",
      "Buzz\n",
      "86\n",
      "Fizz\n",
      "88\n",
      "89\n",
      "FizzBuzz\n",
      "91\n",
      "92\n",
      "Fizz\n",
      "94\n",
      "Buzz\n",
      "Fizz\n",
      "97\n",
      "98\n",
      "Fizz\n",
      "Buzz\n"
     ]
    }
   ],
   "source": [
    "for x in range(1, 101):  # Iteramos del 1 al 100\n",
    "    if x % 3 == 0 and x % 5 == 0:  # Si es múltiplo de 3 y 5\n",
    "        print(\"FizzBuzz\")\n",
    "    elif x % 3 == 0:  # Si es múltiplo de 3\n",
    "        print(\"Fizz\")\n",
    "    elif x % 5 == 0:  # Si es múltiplo de 5\n",
    "        print(\"Buzz\")\n",
    "    else:  # Si no es múltiplo ni de 3 ni de 5\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691df0c0-951d-4f3b-b25d-f23d6151de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leemos el archivo CSV\n",
    "df = pd.read_csv('sales.csv')\n",
    "\n",
    "# Mostramos las primeras filas del dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Creamos la nueva columna 'Total' multiplicando 'Quantity' por 'Price'\n",
    "df['Total'] = df['Quantity'] * df['Price']\n",
    "\n",
    "# Mostramos el dataframe actualizado con la nueva columna 'Total'\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3750915f-04b6-4325-9c0f-18a2f4fe9b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date          Product  Quantity  Price     Category  Sale_ID\n",
      "0  2023-01-01           Laptop         1   1000  Electronics     1001\n",
      "1  2023-01-01          T-Shirt         2     20     Clothing     1002\n",
      "2  2023-01-02  Washing Machine         1    500         Home     1003\n",
      "3  2023-01-02       Headphones         3     50  Electronics     1004\n",
      "4  2023-01-03            Pants         1     30     Clothing     1005\n",
      "         Date          Product  Quantity  Price     Category  Sale_ID  Total\n",
      "0  2023-01-01           Laptop         1   1000  Electronics     1001   1000\n",
      "1  2023-01-01          T-Shirt         2     20     Clothing     1002     40\n",
      "2  2023-01-02  Washing Machine         1    500         Home     1003    500\n",
      "3  2023-01-02       Headphones         3     50  Electronics     1004    150\n",
      "4  2023-01-03            Pants         1     30     Clothing     1005     30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leemos el archivo CSV\n",
    "df = pd.read_csv('sales.csv')\n",
    "\n",
    "# Mostramos las primeras filas del dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Creamos la nueva columna 'Total' multiplicando 'Quantity' por 'Price'\n",
    "df['Total'] = df['Quantity'] * df['Price']\n",
    "\n",
    "# Mostramos el dataframe actualizado con la nueva columna 'Total'\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "945e110e-b75b-4bff-b822-3215aef43e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingreso total de la categoría Electronics: 10395\n",
      "\n",
      "Ingreso total por categoría:\n",
      "Category\n",
      "Clothing        1700\n",
      "Electronics    10395\n",
      "Home            6150\n",
      "Name: Total, dtype: int64\n",
      "\n",
      "Producto con la mayor cantidad total vendida: Laptop\n"
     ]
    }
   ],
   "source": [
    "# Filtramos las ventas de la categoría 'Electronics'\n",
    "df_electronic = df[df['Category'] == 'Electronics']\n",
    "\n",
    "# Calcular el ingreso total de la categoría 'Electronics'\n",
    "total_income_electronics = df_electronic['Total'].sum()\n",
    "print(\"Ingreso total de la categoría Electronics:\", total_income_electronics)\n",
    "\n",
    "# Agrupar las ventas por categoría y calcular el ingreso total para cada una\n",
    "income_by_category = df.groupby('Category')['Total'].sum()\n",
    "print(\"\\nIngreso total por categoría:\")\n",
    "print(income_by_category)\n",
    "\n",
    "# Identificar el producto con la mayor cantidad total vendida\n",
    "product_with_highest_quantity = df.groupby('Product')['Quantity'].sum().idxmax()\n",
    "print(\"\\nProducto con la mayor cantidad total vendida:\", product_with_highest_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c3056e3-d59e-4ae9-91e3-556f9123180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sale ID: 1001, Total: 1000\n",
      "Sale ID: 1002, Total: 40\n",
      "Sale ID: 1003, Total: 500\n",
      "Sale ID: 1004, Total: 150\n",
      "Sale ID: 1005, Total: 30\n",
      "Sale ID: 1006, Total: 800\n",
      "Sale ID: 1007, Total: 200\n",
      "Sale ID: 1008, Total: 100\n",
      "Sale ID: 1009, Total: 700\n",
      "Sale ID: 1010, Total: 120\n",
      "Sale ID: 2001, Total: 2000\n",
      "Sale ID: 2002, Total: 750\n",
      "Sale ID: 2003, Total: 125\n",
      "Sale ID: 2004, Total: 1200\n",
      "Sale ID: 2005, Total: 120\n",
      "Sale ID: 2006, Total: 300\n",
      "Sale ID: 2007, Total: 100\n",
      "Sale ID: 2008, Total: 900\n",
      "Sale ID: 2009, Total: 600\n",
      "Sale ID: 2010, Total: 240\n",
      "Sale ID: 2011, Total: 75\n",
      "Sale ID: 2012, Total: 600\n",
      "Sale ID: 2013, Total: 400\n",
      "Sale ID: 3001, Total: 2850\n",
      "Sale ID: 3002, Total: 280\n",
      "Sale ID: 3003, Total: 700\n",
      "Sale ID: 3004, Total: 600\n",
      "Sale ID: 3005, Total: 165\n",
      "Sale ID: 3006, Total: 150\n",
      "Sale ID: 3007, Total: 600\n",
      "Sale ID: 3008, Total: 200\n",
      "Sale ID: 3009, Total: 450\n",
      "Sale ID: 3010, Total: 1200\n"
     ]
    }
   ],
   "source": [
    "# Usamos un ciclo for para iterar sobre cada fila del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Imprimimos el ID de la venta y el total de esa venta\n",
    "    print(f\"Sale ID: {row['Sale_ID']}, Total: {row['Total']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b88bf1e-fe03-498c-a497-3b27a947841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sale_ID         Product  Total\n",
      "0      1001          Laptop   1000\n",
      "5      1006      Television    800\n",
      "8      1009            Sofa    700\n",
      "10     2001          Laptop   2000\n",
      "11     2002      Television    750\n",
      "13     2004    Refrigerator   1200\n",
      "17     2008           Phone    900\n",
      "18     2009      Dishwasher    600\n",
      "21     2012           Chair    600\n",
      "23     3001          Laptop   2850\n",
      "25     3003  Vacuum Cleaner    700\n",
      "26     3004          Tablet    600\n",
      "29     3007         Monitor    600\n",
      "32     3010             Bed   1200\n"
     ]
    }
   ],
   "source": [
    "# Filtrar las ventas cuyo total es mayor a 500\n",
    "df_high_revenue = df[df['Total'] > 500]\n",
    "\n",
    "# Mostrar los detalles (ID, Product, Total) de las ventas filtradas\n",
    "print(df_high_revenue[['Sale_ID', 'Product', 'Total']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "146a8b6f-81f1-4948-b589-d791c5ee00a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría: Electronics, Total vendido: 30\n",
      "Categoría: Clothing, Total vendido: 24\n",
      "Categoría: Home, Total vendido: 19\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario vacío para almacenar las cantidades por categoría\n",
    "category_sales = {}\n",
    "\n",
    "# Iterar sobre cada fila del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    category = row['Category']  # Obtener la categoría de la venta\n",
    "    quantity = row['Quantity']  # Obtener la cantidad vendida\n",
    "\n",
    "    # Si la categoría ya está en el diccionario, sumamos la cantidad\n",
    "    if category in category_sales:\n",
    "        category_sales[category] += quantity\n",
    "    else:\n",
    "        # Si la categoría no está en el diccionario, la añadimos con la cantidad actual\n",
    "        category_sales[category] = quantity\n",
    "\n",
    "# Imprimir el total de productos vendidos por cada categoría\n",
    "for category, total_quantity in category_sales.items():\n",
    "    print(f\"Categoría: {category}, Total vendido: {total_quantity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ea94b99-63bf-434f-8e23-84bb943578a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ingreso acumulado excede $1000 en el día 2023-01-01\n"
     ]
    }
   ],
   "source": [
    "# Ordenamos el DataFrame por fecha para asegurarnos de que las ventas se procesen en orden\n",
    "df_sorted = df.sort_values(by='Date')\n",
    "\n",
    "# Inicializamos variables\n",
    "accumulated_income = 0\n",
    "current_index = 0\n",
    "\n",
    "# Usamos un ciclo while para acumular el ingreso día por día\n",
    "while accumulated_income <= 1000 and current_index < len(df_sorted):\n",
    "    # Obtenemos la fecha y el total de la venta actual\n",
    "    current_sale = df_sorted.iloc[current_index]\n",
    "    sale_date = current_sale['Date']\n",
    "    sale_total = current_sale['Total']\n",
    "\n",
    "    # Acumulamos el total de la venta\n",
    "    accumulated_income += sale_total\n",
    "    \n",
    "    # Si el ingreso acumulado excede $1000, imprimimos el día\n",
    "    if accumulated_income > 1000:\n",
    "        print(f\"El ingreso acumulado excede $1000 en el día {sale_date}\")\n",
    "        break\n",
    "\n",
    "    # Avanzamos al siguiente índice\n",
    "    current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "111ba5ac-7990-44a2-bbb6-90f23f5a5b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se necesitaron 6 ventas para superar los $2000.\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos variables\n",
    "accumulated_income = 0\n",
    "sales_count = 0\n",
    "\n",
    "# Usamos un ciclo while para acumular el ingreso fila por fila\n",
    "while accumulated_income <= 2000 and sales_count < len(df):\n",
    "    # Obtenemos el total de la venta actual\n",
    "    current_sale_total = df.iloc[sales_count]['Total']\n",
    "    \n",
    "    # Acumulamos el total de la venta\n",
    "    accumulated_income += current_sale_total\n",
    "    \n",
    "    # Aumentamos el contador de ventas\n",
    "    sales_count += 1\n",
    "\n",
    "# Imprimimos cuántas ventas fueron necesarias para superar los $2000\n",
    "if accumulated_income > 2000:\n",
    "    print(f\"Se necesitaron {sales_count} ventas para superar los $2000.\")\n",
    "else:\n",
    "    print(\"No se superaron los $2000 con las ventas disponibles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ee2bb69-fcc9-482c-a010-14d9a78ebda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría: Electronics\n",
      "Ingreso acumulado: $4050\n",
      "Número de transacciones para superar los $3000: 5\n",
      "--------------------------------------------------\n",
      "Categoría: Clothing\n",
      "Ingreso acumulado: $1700\n",
      "Número de transacciones para superar los $3000: 10\n",
      "--------------------------------------------------\n",
      "Categoría: Home\n",
      "Ingreso acumulado: $3500\n",
      "Número de transacciones para superar los $3000: 6\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iteramos sobre cada categoría única\n",
    "for category in df['Category'].unique():\n",
    "    # Filtramos las ventas por categoría\n",
    "    category_sales = df[df['Category'] == category]\n",
    "    \n",
    "    # Inicializamos variables para acumular el ingreso y contar las transacciones\n",
    "    accumulated_income = 0\n",
    "    transaction_count = 0\n",
    "    \n",
    "    # Usamos un ciclo while para acumular las ventas de la categoría\n",
    "    while accumulated_income <= 3000 and transaction_count < len(category_sales):\n",
    "        # Obtenemos el total de la venta de la transacción actual\n",
    "        current_sale_total = category_sales.iloc[transaction_count]['Total']\n",
    "        \n",
    "        # Acumulamos el total de la venta\n",
    "        accumulated_income += current_sale_total\n",
    "        \n",
    "        # Aumentamos el contador de transacciones\n",
    "        transaction_count += 1\n",
    "    \n",
    "    # Imprimimos los resultados para la categoría actual\n",
    "    print(f\"Categoría: {category}\")\n",
    "    print(f\"Ingreso acumulado: ${accumulated_income}\")\n",
    "    print(f\"Número de transacciones para superar los $3000: {transaction_count}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bc97f69-361f-4597-ba46-35f35f87f985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Año: (2023, Timestamp('2023-01-01 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2023-01-01 00:00:00\n",
      "Ingreso total: $1040\n",
      "--------------------------------------------------\n",
      "Año: (2023, Timestamp('2023-01-02 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2023-01-02 00:00:00\n",
      "Ingreso total: $650\n",
      "--------------------------------------------------\n",
      "Año: (2023, Timestamp('2023-01-03 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2023-01-03 00:00:00\n",
      "Ingreso total: $830\n",
      "--------------------------------------------------\n",
      "Año: (2023, Timestamp('2023-01-04 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2023-01-04 00:00:00\n",
      "Ingreso total: $200\n",
      "--------------------------------------------------\n",
      "Año: (2023, Timestamp('2023-01-05 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2023-01-05 00:00:00\n",
      "Ingreso total: $800\n",
      "--------------------------------------------------\n",
      "Año: (2023, Timestamp('2023-01-06 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2023-01-06 00:00:00\n",
      "Ingreso total: $120\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-01-01 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-01-01 00:00:00\n",
      "Ingreso total: $2000\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-01-02 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-01-02 00:00:00\n",
      "Ingreso total: $750\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-01-03 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-01-03 00:00:00\n",
      "Ingreso total: $125\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-01-04 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-01-04 00:00:00\n",
      "Ingreso total: $1320\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-01-05 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-01-05 00:00:00\n",
      "Ingreso total: $300\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-01-06 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-01-06 00:00:00\n",
      "Ingreso total: $100\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-02-01 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-02-01 00:00:00\n",
      "Ingreso total: $900\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-02-02 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-02-02 00:00:00\n",
      "Ingreso total: $600\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-02-03 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-02-03 00:00:00\n",
      "Ingreso total: $240\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-02-04 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-02-04 00:00:00\n",
      "Ingreso total: $75\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-02-05 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-02-05 00:00:00\n",
      "Ingreso total: $600\n",
      "--------------------------------------------------\n",
      "Año: (2024, Timestamp('2024-02-06 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2024-02-06 00:00:00\n",
      "Ingreso total: $400\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-01 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-01 00:00:00\n",
      "Ingreso total: $2850\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-02 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-02 00:00:00\n",
      "Ingreso total: $280\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-03 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-03 00:00:00\n",
      "Ingreso total: $1300\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-04 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-04 00:00:00\n",
      "Ingreso total: $165\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-05 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-05 00:00:00\n",
      "Ingreso total: $150\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-06 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-06 00:00:00\n",
      "Ingreso total: $600\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-07 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-07 00:00:00\n",
      "Ingreso total: $200\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-08 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-08 00:00:00\n",
      "Ingreso total: $450\n",
      "--------------------------------------------------\n",
      "Año: (2025, Timestamp('2025-01-09 00:00:00'))\n",
      "Fecha con el mayor ingreso: 2025-01-09 00:00:00\n",
      "Ingreso total: $1200\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Asegurarnos de que la columna 'Date' esté en formato datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Añadimos una nueva columna para el año\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Agrupamos por 'Year' y luego por 'Date'\n",
    "grouped_by_year_date = df.groupby(['Year', 'Date'])\n",
    "\n",
    "# Iteramos sobre cada año\n",
    "for year, year_group in grouped_by_year_date:\n",
    "    # Calculamos el ingreso total por fecha dentro de ese año\n",
    "    daily_revenue = year_group.groupby('Date')['Total'].sum()\n",
    "    \n",
    "    # Encontramos la fecha con el mayor ingreso para ese año\n",
    "    max_date = daily_revenue.idxmax()  # Fecha con el mayor ingreso\n",
    "    max_revenue = daily_revenue.max()  # Valor del mayor ingreso\n",
    "    \n",
    "    # Imprimimos el resultado para ese año\n",
    "    print(f\"Año: {year}\")\n",
    "    print(f\"Fecha con el mayor ingreso: {max_date}\")\n",
    "    print(f\"Ingreso total: ${max_revenue}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0de548b3-b0b7-4027-ba7e-9b31578b003e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Math</th>\n",
       "      <th>English</th>\n",
       "      <th>Science</th>\n",
       "      <th>History</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>John Doe</td>\n",
       "      <td>2014</td>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>92</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>2014</td>\n",
       "      <td>90</td>\n",
       "      <td>invalid</td>\n",
       "      <td>87</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Emily White</td>\n",
       "      <td>2014</td>\n",
       "      <td>missing</td>\n",
       "      <td>89</td>\n",
       "      <td>94</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Michael Brown</td>\n",
       "      <td>2014</td>\n",
       "      <td>88</td>\n",
       "      <td>84</td>\n",
       "      <td>missing</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Sarah Davis</td>\n",
       "      <td>2014</td>\n",
       "      <td>91</td>\n",
       "      <td>87</td>\n",
       "      <td>invalid</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>Chris Johnson</td>\n",
       "      <td>2014</td>\n",
       "      <td>missing</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>Anna Lee</td>\n",
       "      <td>2014</td>\n",
       "      <td>85</td>\n",
       "      <td>missing</td>\n",
       "      <td>93</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108</td>\n",
       "      <td>David Taylor</td>\n",
       "      <td>2015</td>\n",
       "      <td>82</td>\n",
       "      <td>79</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>109</td>\n",
       "      <td>Emma Wilson</td>\n",
       "      <td>2015</td>\n",
       "      <td>91</td>\n",
       "      <td>86</td>\n",
       "      <td>92</td>\n",
       "      <td>invalid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>Daniel Harris</td>\n",
       "      <td>2015</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>Sophia Moore</td>\n",
       "      <td>2016</td>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "      <td>missing</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112</td>\n",
       "      <td>James Anderson</td>\n",
       "      <td>2016</td>\n",
       "      <td>invalid</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>113</td>\n",
       "      <td>Olivia Thomas</td>\n",
       "      <td>2016</td>\n",
       "      <td>92</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>114</td>\n",
       "      <td>Isabella Jackson</td>\n",
       "      <td>2016</td>\n",
       "      <td>95</td>\n",
       "      <td>invalid</td>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>115</td>\n",
       "      <td>Lucas Martinez</td>\n",
       "      <td>2017</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>116</td>\n",
       "      <td>Mia Garcia</td>\n",
       "      <td>2017</td>\n",
       "      <td>88</td>\n",
       "      <td>invalid</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>117</td>\n",
       "      <td>Matthew Clark</td>\n",
       "      <td>2017</td>\n",
       "      <td>missing</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>118</td>\n",
       "      <td>Charlotte Lewis</td>\n",
       "      <td>2018</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>invalid</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>119</td>\n",
       "      <td>Alexander Young</td>\n",
       "      <td>2018</td>\n",
       "      <td>invalid</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>120</td>\n",
       "      <td>Amelia King</td>\n",
       "      <td>2018</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>121</td>\n",
       "      <td>Ethan Wright</td>\n",
       "      <td>2019</td>\n",
       "      <td>missing</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>122</td>\n",
       "      <td>Harper Scott</td>\n",
       "      <td>2019</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>123</td>\n",
       "      <td>Benjamin Green</td>\n",
       "      <td>2019</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>124</td>\n",
       "      <td>Evelyn Hall</td>\n",
       "      <td>2019</td>\n",
       "      <td>missing</td>\n",
       "      <td>invalid</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>125</td>\n",
       "      <td>Avery Adams</td>\n",
       "      <td>2020</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>126</td>\n",
       "      <td>Jackson Baker</td>\n",
       "      <td>2020</td>\n",
       "      <td>invalid</td>\n",
       "      <td>85</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>127</td>\n",
       "      <td>Lily Nelson</td>\n",
       "      <td>2020</td>\n",
       "      <td>missing</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>128</td>\n",
       "      <td>Elijah Carter</td>\n",
       "      <td>2020</td>\n",
       "      <td>94</td>\n",
       "      <td>88</td>\n",
       "      <td>96</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>129</td>\n",
       "      <td>Zoey Perez</td>\n",
       "      <td>2021</td>\n",
       "      <td>92</td>\n",
       "      <td>missing</td>\n",
       "      <td>invalid</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>130</td>\n",
       "      <td>Aiden Mitchell</td>\n",
       "      <td>2021</td>\n",
       "      <td>90</td>\n",
       "      <td>invalid</td>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>131</td>\n",
       "      <td>Layla Roberts</td>\n",
       "      <td>2021</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>132</td>\n",
       "      <td>Owen Ramirez</td>\n",
       "      <td>2021</td>\n",
       "      <td>invalid</td>\n",
       "      <td>88</td>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>133</td>\n",
       "      <td>Hannah Walker</td>\n",
       "      <td>2022</td>\n",
       "      <td>94</td>\n",
       "      <td>missing</td>\n",
       "      <td>94</td>\n",
       "      <td>invalid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>134</td>\n",
       "      <td>Logan Collins</td>\n",
       "      <td>2022</td>\n",
       "      <td>88</td>\n",
       "      <td>84</td>\n",
       "      <td>missing</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>135</td>\n",
       "      <td>Abigail Sanders</td>\n",
       "      <td>2022</td>\n",
       "      <td>missing</td>\n",
       "      <td>89</td>\n",
       "      <td>92</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>136</td>\n",
       "      <td>Jacob Edwards</td>\n",
       "      <td>2022</td>\n",
       "      <td>91</td>\n",
       "      <td>87</td>\n",
       "      <td>95</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>137</td>\n",
       "      <td>Scarlett Murphy</td>\n",
       "      <td>2022</td>\n",
       "      <td>invalid</td>\n",
       "      <td>invalid</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>138</td>\n",
       "      <td>Mason Barnes</td>\n",
       "      <td>2022</td>\n",
       "      <td>92</td>\n",
       "      <td>88</td>\n",
       "      <td>96</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>139</td>\n",
       "      <td>Aria Bell</td>\n",
       "      <td>2022</td>\n",
       "      <td>89</td>\n",
       "      <td>missing</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>140</td>\n",
       "      <td>Eleanor Fisher</td>\n",
       "      <td>2022</td>\n",
       "      <td>missing</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>141</td>\n",
       "      <td>Grayson Powell</td>\n",
       "      <td>2022</td>\n",
       "      <td>missing</td>\n",
       "      <td>88</td>\n",
       "      <td>94</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>142</td>\n",
       "      <td>Emily Hughes</td>\n",
       "      <td>2022</td>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>missing</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>143</td>\n",
       "      <td>Michael Foster</td>\n",
       "      <td>2022</td>\n",
       "      <td>missing</td>\n",
       "      <td>invalid</td>\n",
       "      <td>91</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>144</td>\n",
       "      <td>Harper Simmons</td>\n",
       "      <td>2022</td>\n",
       "      <td>95</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Student_ID              Name  Year     Math  English  Science  History\n",
       "0          101          John Doe  2014       85       78       92       88\n",
       "1          102        Jane Smith  2014       90  invalid       87       82\n",
       "2          103       Emily White  2014  missing       89       94       85\n",
       "3          104     Michael Brown  2014       88       84  missing       79\n",
       "4          105       Sarah Davis  2014       91       87  invalid  missing\n",
       "5          106     Chris Johnson  2014  missing       78       90       81\n",
       "6          107          Anna Lee  2014       85  missing       93  missing\n",
       "7          108      David Taylor  2015       82       79       95       88\n",
       "8          109       Emma Wilson  2015       91       86       92  invalid\n",
       "9          110     Daniel Harris  2015  missing  missing       93       89\n",
       "10         111      Sophia Moore  2016       87       85  missing       84\n",
       "11         112    James Anderson  2016  invalid       90       92       80\n",
       "12         113     Olivia Thomas  2016       92       87       94       91\n",
       "13         114  Isabella Jackson  2016       95  invalid       96       93\n",
       "14         115    Lucas Martinez  2017       90       88       97       90\n",
       "15         116        Mia Garcia  2017       88  invalid       95       91\n",
       "16         117     Matthew Clark  2017  missing       92       94       93\n",
       "17         118   Charlotte Lewis  2018       89       90  invalid  missing\n",
       "18         119   Alexander Young  2018  invalid       88       93       94\n",
       "19         120       Amelia King  2018       92       91       92       95\n",
       "20         121      Ethan Wright  2019  missing       90       89  missing\n",
       "21         122      Harper Scott  2019       94       87       92       96\n",
       "22         123    Benjamin Green  2019       88       85  missing  missing\n",
       "23         124       Evelyn Hall  2019  missing  invalid       91       92\n",
       "24         125       Avery Adams  2020       93       89       92       93\n",
       "25         126     Jackson Baker  2020  invalid       85       94       96\n",
       "26         127       Lily Nelson  2020  missing       90       93       89\n",
       "27         128     Elijah Carter  2020       94       88       96       92\n",
       "28         129        Zoey Perez  2021       92  missing  invalid       91\n",
       "29         130    Aiden Mitchell  2021       90  invalid       93       88\n",
       "30         131     Layla Roberts  2021       85       86       92       94\n",
       "31         132      Owen Ramirez  2021  invalid       88       91       93\n",
       "32         133     Hannah Walker  2022       94  missing       94  invalid\n",
       "33         134     Logan Collins  2022       88       84  missing       91\n",
       "34         135   Abigail Sanders  2022  missing       89       92  missing\n",
       "35         136     Jacob Edwards  2022       91       87       95       92\n",
       "36         137   Scarlett Murphy  2022  invalid  invalid       93       95\n",
       "37         138      Mason Barnes  2022       92       88       96       94\n",
       "38         139         Aria Bell  2022       89  missing       94       93\n",
       "39         140    Eleanor Fisher  2022  missing       90       92       91\n",
       "40         141    Grayson Powell  2022  missing       88       94       89\n",
       "41         142      Emily Hughes  2022       93       92  missing       90\n",
       "42         143    Michael Foster  2022  missing  invalid       91  missing\n",
       "43         144    Harper Simmons  2022       95       89       93       94"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "file_students_grade=pd.read_csv(\"students_grade.csv\")\n",
    "file_students_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faf44c84-75ed-4241-8ca6-a86fc93a16dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Student_ID              Name  Year     Math  English  Science  History  \\\n",
      "0          101          John Doe  2014       85       78       92       88   \n",
      "1          102        Jane Smith  2014       90  invalid       87       82   \n",
      "2          103       Emily White  2014  missing       89       94       85   \n",
      "3          104     Michael Brown  2014       88       84  missing       79   \n",
      "4          105       Sarah Davis  2014       91       87  invalid  missing   \n",
      "5          106     Chris Johnson  2014  missing       78       90       81   \n",
      "6          107          Anna Lee  2014       85  missing       93  missing   \n",
      "7          108      David Taylor  2015       82       79       95       88   \n",
      "8          109       Emma Wilson  2015       91       86       92  invalid   \n",
      "9          110     Daniel Harris  2015  missing  missing       93       89   \n",
      "10         111      Sophia Moore  2016       87       85  missing       84   \n",
      "11         112    James Anderson  2016  invalid       90       92       80   \n",
      "12         113     Olivia Thomas  2016       92       87       94       91   \n",
      "13         114  Isabella Jackson  2016       95  invalid       96       93   \n",
      "14         115    Lucas Martinez  2017       90       88       97       90   \n",
      "15         116        Mia Garcia  2017       88  invalid       95       91   \n",
      "16         117     Matthew Clark  2017  missing       92       94       93   \n",
      "17         118   Charlotte Lewis  2018       89       90  invalid  missing   \n",
      "18         119   Alexander Young  2018  invalid       88       93       94   \n",
      "19         120       Amelia King  2018       92       91       92       95   \n",
      "20         121      Ethan Wright  2019  missing       90       89  missing   \n",
      "21         122      Harper Scott  2019       94       87       92       96   \n",
      "22         123    Benjamin Green  2019       88       85  missing  missing   \n",
      "23         124       Evelyn Hall  2019  missing  invalid       91       92   \n",
      "24         125       Avery Adams  2020       93       89       92       93   \n",
      "25         126     Jackson Baker  2020  invalid       85       94       96   \n",
      "26         127       Lily Nelson  2020  missing       90       93       89   \n",
      "27         128     Elijah Carter  2020       94       88       96       92   \n",
      "28         129        Zoey Perez  2021       92  missing  invalid       91   \n",
      "29         130    Aiden Mitchell  2021       90  invalid       93       88   \n",
      "30         131     Layla Roberts  2021       85       86       92       94   \n",
      "31         132      Owen Ramirez  2021  invalid       88       91       93   \n",
      "32         133     Hannah Walker  2022       94  missing       94  invalid   \n",
      "33         134     Logan Collins  2022       88       84  missing       91   \n",
      "34         135   Abigail Sanders  2022  missing       89       92  missing   \n",
      "35         136     Jacob Edwards  2022       91       87       95       92   \n",
      "36         137   Scarlett Murphy  2022  invalid  invalid       93       95   \n",
      "37         138      Mason Barnes  2022       92       88       96       94   \n",
      "38         139         Aria Bell  2022       89  missing       94       93   \n",
      "39         140    Eleanor Fisher  2022  missing       90       92       91   \n",
      "40         141    Grayson Powell  2022  missing       88       94       89   \n",
      "41         142      Emily Hughes  2022       93       92  missing       90   \n",
      "42         143    Michael Foster  2022  missing  invalid       91  missing   \n",
      "43         144    Harper Simmons  2022       95       89       93       94   \n",
      "\n",
      "      Average  \n",
      "0   85.750000  \n",
      "1   86.333333  \n",
      "2   89.333333  \n",
      "3   83.666667  \n",
      "4   89.000000  \n",
      "5   83.000000  \n",
      "6   89.000000  \n",
      "7   86.000000  \n",
      "8   89.666667  \n",
      "9   91.000000  \n",
      "10  85.333333  \n",
      "11  87.333333  \n",
      "12  91.000000  \n",
      "13  94.666667  \n",
      "14  91.250000  \n",
      "15  91.333333  \n",
      "16  93.000000  \n",
      "17  89.500000  \n",
      "18  91.666667  \n",
      "19  92.500000  \n",
      "20  89.500000  \n",
      "21  92.250000  \n",
      "22  86.500000  \n",
      "23  91.500000  \n",
      "24  91.750000  \n",
      "25  91.666667  \n",
      "26  90.666667  \n",
      "27  92.500000  \n",
      "28  91.500000  \n",
      "29  90.333333  \n",
      "30  89.250000  \n",
      "31  90.666667  \n",
      "32  94.000000  \n",
      "33  87.666667  \n",
      "34  90.500000  \n",
      "35  91.250000  \n",
      "36  94.000000  \n",
      "37  92.500000  \n",
      "38  92.000000  \n",
      "39  91.000000  \n",
      "40  90.333333  \n",
      "41  91.666667  \n",
      "42  91.000000  \n",
      "43  92.750000  \n",
      "John Doe: Avg grade = 85.75\n",
      "Jane Smith: Avg grade = 86.33\n",
      "Emily White: Avg grade = 89.33\n",
      "Michael Brown: Avg grade = 83.67\n",
      "Sarah Davis: Avg grade = 89.00\n",
      "Chris Johnson: Avg grade = 83.00\n",
      "Anna Lee: Avg grade = 89.00\n",
      "David Taylor: Avg grade = 86.00\n",
      "Emma Wilson: Avg grade = 89.67\n",
      "Daniel Harris: Avg grade = 91.00\n",
      "Sophia Moore: Avg grade = 85.33\n",
      "James Anderson: Avg grade = 87.33\n",
      "Olivia Thomas: Avg grade = 91.00\n",
      "Isabella Jackson: Avg grade = 94.67\n",
      "Lucas Martinez: Avg grade = 91.25\n",
      "Mia Garcia: Avg grade = 91.33\n",
      "Matthew Clark: Avg grade = 93.00\n",
      "Charlotte Lewis: Avg grade = 89.50\n",
      "Alexander Young: Avg grade = 91.67\n",
      "Amelia King: Avg grade = 92.50\n",
      "Ethan Wright: Avg grade = 89.50\n",
      "Harper Scott: Avg grade = 92.25\n",
      "Benjamin Green: Avg grade = 86.50\n",
      "Evelyn Hall: Avg grade = 91.50\n",
      "Avery Adams: Avg grade = 91.75\n",
      "Jackson Baker: Avg grade = 91.67\n",
      "Lily Nelson: Avg grade = 90.67\n",
      "Elijah Carter: Avg grade = 92.50\n",
      "Zoey Perez: Avg grade = 91.50\n",
      "Aiden Mitchell: Avg grade = 90.33\n",
      "Layla Roberts: Avg grade = 89.25\n",
      "Owen Ramirez: Avg grade = 90.67\n",
      "Hannah Walker: Avg grade = 94.00\n",
      "Logan Collins: Avg grade = 87.67\n",
      "Abigail Sanders: Avg grade = 90.50\n",
      "Jacob Edwards: Avg grade = 91.25\n",
      "Scarlett Murphy: Avg grade = 94.00\n",
      "Mason Barnes: Avg grade = 92.50\n",
      "Aria Bell: Avg grade = 92.00\n",
      "Eleanor Fisher: Avg grade = 91.00\n",
      "Grayson Powell: Avg grade = 90.33\n",
      "Emily Hughes: Avg grade = 91.67\n",
      "Michael Foster: Avg grade = 91.00\n",
      "Harper Simmons: Avg grade = 92.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definir una función para calcular el promedio por estudiante\n",
    "def calculate_average(row):\n",
    "    # Filtrar las calificaciones válidas (numéricas) directamente\n",
    "    grades = row[[\"Math\", \"English\", \"Science\", \"History\"]].apply(pd.to_numeric, errors='coerce')\n",
    "    valid_grades = grades.dropna()  # Eliminar valores no válidos (NaN)\n",
    "    \n",
    "    # Devolver el promedio si hay calificaciones válidas, o None si no\n",
    "    return valid_grades.mean() if not valid_grades.empty else None\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame para calcular el promedio\n",
    "file_students_grade[\"Average\"] = file_students_grade.apply(calculate_average, axis=1)\n",
    "\n",
    "# Mostrar el DataFrame con la nueva columna de promedios\n",
    "print(file_students_grade)\n",
    "\n",
    "# registrar el promedio por estudiante\n",
    "for student, average in file_students_grade[[\"Name\", \"Average\"]].dropna().values:\n",
    "    print(f\"{student}: Avg grade = {average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "788b7fb6-2701-4cfe-afc6-fde79f2087ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average grades per subject by year:\n",
      "           Math    English    Science    History\n",
      "Year                                            \n",
      "2014  87.800000  83.200000  91.200000  83.000000\n",
      "2015  86.500000  82.500000  93.333333  88.500000\n",
      "2016  91.333333  87.333333  94.000000  87.000000\n",
      "2017  89.000000  90.000000  95.333333  91.333333\n",
      "2018  90.500000  89.666667  92.500000  94.500000\n",
      "2019  91.000000  87.333333  90.666667  94.000000\n",
      "2020  93.500000  88.000000  93.750000  92.500000\n",
      "2021  89.000000  87.000000  92.000000  91.500000\n",
      "2022  91.714286  88.375000  93.400000  92.111111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supongamos que 'file_students_grade' tiene las columnas 'Year', 'Math', 'English', 'Science', 'History'\n",
    "# Además de otras posibles columnas como 'Name', etc.\n",
    "\n",
    "# Filtrar y convertir las calificaciones a numéricas, gestionando valores inválidos (coerción a NaN)\n",
    "subjects = [\"Math\", \"English\", \"Science\", \"History\"]\n",
    "\n",
    "# Asegurarse de que las calificaciones son numéricas\n",
    "file_students_grade[subjects] = file_students_grade[subjects].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calcular el promedio por asignatura para cada año\n",
    "average_by_year = file_students_grade.groupby(\"Year\")[subjects].mean()\n",
    "\n",
    "# Mostrar el DataFrame con los promedios por asignatura para cada año\n",
    "print(\"Average grades per subject by year:\")\n",
    "print(average_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37f8a559-d875-4953-a2af-9bd54f92876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Students with an average grade of 90 or above:\n",
      "Daniel Harris: Avg grade = 91.00\n",
      "Olivia Thomas: Avg grade = 91.00\n",
      "Isabella Jackson: Avg grade = 94.67\n",
      "Lucas Martinez: Avg grade = 91.25\n",
      "Mia Garcia: Avg grade = 91.33\n",
      "Matthew Clark: Avg grade = 93.00\n",
      "Alexander Young: Avg grade = 91.67\n",
      "Amelia King: Avg grade = 92.50\n",
      "Harper Scott: Avg grade = 92.25\n",
      "Evelyn Hall: Avg grade = 91.50\n",
      "Avery Adams: Avg grade = 91.75\n",
      "Jackson Baker: Avg grade = 91.67\n",
      "Lily Nelson: Avg grade = 90.67\n",
      "Elijah Carter: Avg grade = 92.50\n",
      "Zoey Perez: Avg grade = 91.50\n",
      "Aiden Mitchell: Avg grade = 90.33\n",
      "Owen Ramirez: Avg grade = 90.67\n",
      "Hannah Walker: Avg grade = 94.00\n",
      "Abigail Sanders: Avg grade = 90.50\n",
      "Jacob Edwards: Avg grade = 91.25\n",
      "Scarlett Murphy: Avg grade = 94.00\n",
      "Mason Barnes: Avg grade = 92.50\n",
      "Aria Bell: Avg grade = 92.00\n",
      "Eleanor Fisher: Avg grade = 91.00\n",
      "Grayson Powell: Avg grade = 90.33\n",
      "Emily Hughes: Avg grade = 91.67\n",
      "Michael Foster: Avg grade = 91.00\n",
      "Harper Simmons: Avg grade = 92.75\n"
     ]
    }
   ],
   "source": [
    "# Filtrar y convertir las calificaciones a numéricas, gestionando valores inválidos (coerción a NaN)\n",
    "subjects = [\"Math\", \"English\", \"Science\", \"History\"]\n",
    "file_students_grade[subjects] = file_students_grade[subjects].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calcular el promedio por estudiante\n",
    "file_students_grade['Average'] = file_students_grade[subjects].mean(axis=1)\n",
    "\n",
    "# Filtrar a los estudiantes con promedio >= 90 y calificaciones válidas\n",
    "students_above_90 = file_students_grade[file_students_grade['Average'] >= 90]\n",
    "\n",
    "# Identificar estudiantes con promedio válido\n",
    "valid_students = file_students_grade.dropna(subset=['Average'])\n",
    "\n",
    "# Verificación de estudiantes sin calificaciones válidas\n",
    "for index, row in file_students_grade.iterrows():\n",
    "    if pd.isna(row['Average']):\n",
    "        print(f\"Warning: {row['Name']} has no valid grades.\")\n",
    "\n",
    "# Mostrar los estudiantes con promedio >= 90\n",
    "if not students_above_90.empty:\n",
    "    print(\"\\nStudents with an average grade of 90 or above:\")\n",
    "    for index, row in students_above_90.iterrows():\n",
    "        print(f\"{row['Name']}: Avg grade = {row['Average']:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo students have an average of 90 or above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bc33d-04de-4596-bf14-7aa0892118bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
